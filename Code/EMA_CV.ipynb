{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import process\n",
    "import numpy as np \n",
    "# Jerome path : r'C:\\Users\\33640\\OneDrive\\Documents\\GitHub\\Portfolio_clustering_project\\Data\\DataBase.csv'\n",
    "# Nail path : '/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DataBase.csv'\n",
    "df = pd.read_csv(r'/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Data/DataBase.csv')\n",
    "\n",
    "df.set_index('ticker', inplace=True)\n",
    "\n",
    "df.columns = pd.to_datetime(df.columns.str[1:], format='%Y%m%d').strftime('%d/%m/%Y')\n",
    "\n",
    "df_cleaned = df.fillna(0) # Utilisez la méthode fillna(0) pour remplacer les NaN par 0\n",
    "\n",
    "df_cleaned = df_cleaned.transpose() ## WE WANT COLUMNS TO BE VECTOR OF RETURN FOR A GIVEN TICKER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote by $\\bm{X}$ the $d \\times n$ matrix of observations \\textit{i.e.}\n",
    "\n",
    "$$\n",
    "\\bm{X} = \n",
    "\\begin{bmatrix}\n",
    "    r_{1}^{(1)} & r_{1}^{(2)} & \\ldots & r_{1}^{(n)} \\\\\n",
    "    r_{2}^{(1)} & r_{2}^{(2)} & \\ldots & r_{2}^{(n)} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    r_{d}^{(1)} & r_{d}^{(2)} & \\ldots & r_{d}^{(n)} \\\\\n",
    "\\end{bmatrix} \n",
    "= \\begin{bmatrix} \\mathbf{r}^{(1)} | ... |\\mathbf{r}^{(n)}\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{d\\times n}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $d$ corresponds to the number of days\n",
    "- $n$ corresponds to the number of stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a standard sample covariance can be generalized to include some arbitrary weight profile assigned along the time dimension. In particular, it is expressed in the following form\n",
    "\n",
    "\\begin{equation}\n",
    "\\bm{S_W} := \\frac{1}{d} \\bm{X}'W\\bm{X} \\in \\mathbb{R}^{n\\times n}\n",
    "\\end{equation}\\\\\n",
    "\n",
    "The EWA-SC as defined can be written as a weighted sample covariance matrix if we define the matrix of weighted $W$ to be: \n",
    "\n",
    "\\begin{align*}\n",
    "    W_{t,k} =\n",
    "    \\begin{cases}\n",
    "        d\\frac{1 - \\beta}{1-\\beta^d} \\beta^{d-t} & \\text{if } t = k \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "If we define the auxiliary observation matrix: \n",
    "\n",
    "\\begin{align}\n",
    "    \\tilde{\\bm{{X}}} := \\bm{W}^\\frac{1}{2} \\bm{X}\n",
    "\\end{align}\n",
    "\n",
    "then we can see that the EWA-SC can be expressed in a similar form as the standard uniformly weighted sample covariance. The advantage of recasting the EWA-SC in this way is that many of the refinements for the standard sample covariance that have been developed over the years are at our disposal; including shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>03/01/2000</th>\n",
       "      <th>04/01/2000</th>\n",
       "      <th>05/01/2000</th>\n",
       "      <th>06/01/2000</th>\n",
       "      <th>07/01/2000</th>\n",
       "      <th>10/01/2000</th>\n",
       "      <th>11/01/2000</th>\n",
       "      <th>12/01/2000</th>\n",
       "      <th>13/01/2000</th>\n",
       "      <th>14/01/2000</th>\n",
       "      <th>...</th>\n",
       "      <th>13/12/2000</th>\n",
       "      <th>14/12/2000</th>\n",
       "      <th>15/12/2000</th>\n",
       "      <th>18/12/2000</th>\n",
       "      <th>19/12/2000</th>\n",
       "      <th>20/12/2000</th>\n",
       "      <th>21/12/2000</th>\n",
       "      <th>22/12/2000</th>\n",
       "      <th>26/12/2000</th>\n",
       "      <th>27/12/2000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AA</th>\n",
       "      <td>-0.012239</td>\n",
       "      <td>0.009429</td>\n",
       "      <td>0.044739</td>\n",
       "      <td>-0.011008</td>\n",
       "      <td>-0.015155</td>\n",
       "      <td>-0.030173</td>\n",
       "      <td>0.021279</td>\n",
       "      <td>-0.004943</td>\n",
       "      <td>-0.017157</td>\n",
       "      <td>-0.018955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037058</td>\n",
       "      <td>-0.002082</td>\n",
       "      <td>-0.008524</td>\n",
       "      <td>0.018609</td>\n",
       "      <td>0.040317</td>\n",
       "      <td>-0.051453</td>\n",
       "      <td>0.012609</td>\n",
       "      <td>0.072985</td>\n",
       "      <td>-0.007656</td>\n",
       "      <td>-0.009671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABM</th>\n",
       "      <td>-0.008622</td>\n",
       "      <td>0.011591</td>\n",
       "      <td>-0.005816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.008755</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>-0.026964</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>-0.004303</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.015092</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>-0.006436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052943</td>\n",
       "      <td>-0.029570</td>\n",
       "      <td>0.004304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABT</th>\n",
       "      <td>-0.006679</td>\n",
       "      <td>-0.012004</td>\n",
       "      <td>0.010437</td>\n",
       "      <td>0.030593</td>\n",
       "      <td>0.026866</td>\n",
       "      <td>-0.019806</td>\n",
       "      <td>0.010212</td>\n",
       "      <td>-0.020509</td>\n",
       "      <td>-0.008684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023227</td>\n",
       "      <td>-0.048372</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>0.035326</td>\n",
       "      <td>0.026674</td>\n",
       "      <td>-0.025497</td>\n",
       "      <td>-0.013694</td>\n",
       "      <td>-0.018056</td>\n",
       "      <td>0.026508</td>\n",
       "      <td>-0.005458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADI</th>\n",
       "      <td>-0.033849</td>\n",
       "      <td>-0.041555</td>\n",
       "      <td>0.013614</td>\n",
       "      <td>-0.026050</td>\n",
       "      <td>0.031644</td>\n",
       "      <td>0.045277</td>\n",
       "      <td>-0.030045</td>\n",
       "      <td>0.032663</td>\n",
       "      <td>-0.019261</td>\n",
       "      <td>0.053811</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100518</td>\n",
       "      <td>-0.026081</td>\n",
       "      <td>0.020950</td>\n",
       "      <td>-0.057097</td>\n",
       "      <td>0.036648</td>\n",
       "      <td>-0.003907</td>\n",
       "      <td>-0.057160</td>\n",
       "      <td>0.012197</td>\n",
       "      <td>0.012939</td>\n",
       "      <td>0.070431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>-0.014950</td>\n",
       "      <td>0.010051</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>-0.004913</td>\n",
       "      <td>-0.014900</td>\n",
       "      <td>0.019722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010586</td>\n",
       "      <td>-0.010234</td>\n",
       "      <td>0.030135</td>\n",
       "      <td>0.004875</td>\n",
       "      <td>-0.009867</td>\n",
       "      <td>0.043721</td>\n",
       "      <td>-0.004794</td>\n",
       "      <td>0.014324</td>\n",
       "      <td>0.018730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLY</th>\n",
       "      <td>-0.026868</td>\n",
       "      <td>-0.014942</td>\n",
       "      <td>-0.015635</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.033271</td>\n",
       "      <td>-0.003860</td>\n",
       "      <td>-0.008233</td>\n",
       "      <td>-0.007799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003414</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020053</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>-0.008340</td>\n",
       "      <td>0.013062</td>\n",
       "      <td>-0.028330</td>\n",
       "      <td>0.021488</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>0.013630</td>\n",
       "      <td>-0.010994</td>\n",
       "      <td>0.043059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XOM</th>\n",
       "      <td>-0.015621</td>\n",
       "      <td>-0.006850</td>\n",
       "      <td>0.035450</td>\n",
       "      <td>0.049661</td>\n",
       "      <td>-0.011006</td>\n",
       "      <td>-0.004901</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.019541</td>\n",
       "      <td>-0.019494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011374</td>\n",
       "      <td>-0.015365</td>\n",
       "      <td>-0.004711</td>\n",
       "      <td>0.026397</td>\n",
       "      <td>0.012999</td>\n",
       "      <td>-0.025376</td>\n",
       "      <td>-0.003127</td>\n",
       "      <td>0.023966</td>\n",
       "      <td>0.021202</td>\n",
       "      <td>-0.012793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XRX</th>\n",
       "      <td>0.032064</td>\n",
       "      <td>-0.044716</td>\n",
       "      <td>0.020201</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>0.022010</td>\n",
       "      <td>-0.038799</td>\n",
       "      <td>-0.014900</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>0.022286</td>\n",
       "      <td>-0.024678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035471</td>\n",
       "      <td>0.087359</td>\n",
       "      <td>0.032261</td>\n",
       "      <td>0.020985</td>\n",
       "      <td>0.051229</td>\n",
       "      <td>-0.043302</td>\n",
       "      <td>-0.117316</td>\n",
       "      <td>0.053774</td>\n",
       "      <td>-0.026896</td>\n",
       "      <td>0.040624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YUM</th>\n",
       "      <td>-0.030922</td>\n",
       "      <td>-0.011168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001611</td>\n",
       "      <td>-0.021204</td>\n",
       "      <td>0.038866</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>-0.014426</td>\n",
       "      <td>0.003188</td>\n",
       "      <td>-0.025827</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028318</td>\n",
       "      <td>-0.029058</td>\n",
       "      <td>-0.007890</td>\n",
       "      <td>0.048308</td>\n",
       "      <td>0.009407</td>\n",
       "      <td>-0.051475</td>\n",
       "      <td>-0.050307</td>\n",
       "      <td>-0.010339</td>\n",
       "      <td>-0.010426</td>\n",
       "      <td>0.022858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTR</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>0.008825</td>\n",
       "      <td>0.008667</td>\n",
       "      <td>0.008671</td>\n",
       "      <td>-0.035190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009016</td>\n",
       "      <td>-0.009020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007867</td>\n",
       "      <td>-0.014274</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.012665</td>\n",
       "      <td>-0.009576</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>-0.016048</td>\n",
       "      <td>0.008070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>695 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        03/01/2000  04/01/2000  05/01/2000  06/01/2000  07/01/2000  \\\n",
       "ticker                                                               \n",
       "AA       -0.012239    0.009429    0.044739   -0.011008   -0.015155   \n",
       "ABM      -0.008622    0.011591   -0.005816    0.000000    0.002906   \n",
       "ABT      -0.006679   -0.012004    0.010437    0.030593    0.026866   \n",
       "ADI      -0.033849   -0.041555    0.013614   -0.026050    0.031644   \n",
       "ADM       0.000000    0.004954   -0.014950    0.010051    0.004936   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "XLY      -0.026868   -0.014942   -0.015635    0.004538    0.033271   \n",
       "XOM      -0.015621   -0.006850    0.035450    0.049661   -0.011006   \n",
       "XRX       0.032064   -0.044716    0.020201    0.007449    0.022010   \n",
       "YUM      -0.030922   -0.011168    0.000000   -0.001611   -0.021204   \n",
       "ZTR       0.000000    0.017715    0.008904    0.008825    0.008667   \n",
       "\n",
       "        10/01/2000  11/01/2000  12/01/2000  13/01/2000  14/01/2000  ...  \\\n",
       "ticker                                                              ...   \n",
       "AA       -0.030173    0.021279   -0.004943   -0.017157   -0.018955  ...   \n",
       "ABM       0.000000   -0.008755    0.002947   -0.026964    0.011710  ...   \n",
       "ABT      -0.019806    0.010212   -0.020509   -0.008684    0.000000  ...   \n",
       "ADI       0.045277   -0.030045    0.032663   -0.019261    0.053811  ...   \n",
       "ADM      -0.004913   -0.014900    0.019722    0.000000    0.028712  ...   \n",
       "...            ...         ...         ...         ...         ...  ...   \n",
       "XLY      -0.003860   -0.008233   -0.007799    0.000000   -0.003414  ...   \n",
       "XOM      -0.004901    0.002806    0.002824    0.019541   -0.019494  ...   \n",
       "XRX      -0.038799   -0.014900    0.007564    0.022286   -0.024678  ...   \n",
       "YUM       0.038866    0.001599   -0.014426    0.003188   -0.025827  ...   \n",
       "ZTR       0.008671   -0.035190    0.000000   -0.009016   -0.009020  ...   \n",
       "\n",
       "        13/12/2000  14/12/2000  15/12/2000  18/12/2000  19/12/2000  \\\n",
       "ticker                                                               \n",
       "AA        0.037058   -0.002082   -0.008524    0.018609    0.040317   \n",
       "ABM       0.017241   -0.004303    0.002164    0.015092    0.008568   \n",
       "ABT       0.023227   -0.048372    0.030120    0.035326    0.026674   \n",
       "ADI      -0.100518   -0.026081    0.020950   -0.057097    0.036648   \n",
       "ADM       0.000000    0.010586   -0.010234    0.030135    0.004875   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "XLY      -0.020053    0.000678   -0.008340    0.013062   -0.028330   \n",
       "XOM       0.011374   -0.015365   -0.004711    0.026397    0.012999   \n",
       "XRX       0.035471    0.087359    0.032261    0.020985    0.051229   \n",
       "YUM      -0.028318   -0.029058   -0.007890    0.048308    0.009407   \n",
       "ZTR      -0.007867   -0.014274    0.004775    0.001599    0.004787   \n",
       "\n",
       "        20/12/2000  21/12/2000  22/12/2000  26/12/2000  27/12/2000  \n",
       "ticker                                                              \n",
       "AA       -0.051453    0.012609    0.072985   -0.007656   -0.009671  \n",
       "ABM      -0.006436    0.000000    0.052943   -0.029570    0.004304  \n",
       "ABT      -0.025497   -0.013694   -0.018056    0.026508   -0.005458  \n",
       "ADI      -0.003907   -0.057160    0.012197    0.012939    0.070431  \n",
       "ADM      -0.009867    0.043721   -0.004794    0.014324    0.018730  \n",
       "...            ...         ...         ...         ...         ...  \n",
       "XLY       0.021488    0.033561    0.013630   -0.010994    0.043059  \n",
       "XOM      -0.025376   -0.003127    0.023966    0.021202   -0.012793  \n",
       "XRX      -0.043302   -0.117316    0.053774   -0.026896    0.040624  \n",
       "YUM      -0.051475   -0.050307   -0.010339   -0.010426    0.022858  \n",
       "ZTR      -0.012665   -0.009576    0.004800   -0.016048    0.008070  \n",
       "\n",
       "[695 rows x 250 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################### 1. We start by randomizing the auxiliary observation matrix  ̃X from Equation (5) along the time axis #########################\n",
    "def auxilary_matrix(days, beta, df_cleaned):\n",
    "\n",
    "    ## 1. We extract the data corresponding to the returns of our assets (columns) during these d days (lines)\n",
    "    X = df_cleaned.iloc[0:days,:] ## shape days * number of stocks\n",
    "\n",
    "    ## 2. We slightly adjust the matrix of observations to get the auxiliary matrix that puts more weight on recent dates\n",
    "\n",
    "    W = np.sqrt(np.diag(days * (1 - beta) * beta**(np.arange(days)[::-1]) / (1 - beta**days)))  # Compute the weight matrix\n",
    "    X_tilde = pd.DataFrame(index=X.index, columns=X.columns, data=np.dot(W, X)).transpose()\n",
    "\n",
    "    ## 3. We randomize the auxiliary matrix of observations according to the time axis\n",
    "    # Randomized_X = X_tilde.transpose().sample(frac=1, axis=1, random_state=42) ## we transpose X as we want to have daily observations of the whole dataset !\n",
    "\n",
    "    return X_tilde\n",
    "\n",
    "# ---------------------------------------------------------------- TESTS ----------------------------------------------------------------\n",
    "\n",
    "days = 250\n",
    "beta = 0.999\n",
    "X_tilde = auxilary_matrix(days=days, beta=beta, df_cleaned=df_cleaned)\n",
    "X_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the (randomized) auxiliary observations into $K$ non-overlapping folds **of equal size** represented as $\\{\\mathcal{I}_k | \\mathcal{I}_k \\subset \\{1, ..., K\\}\\}_{k=1}^K$. Each set indexed by $\\mathcal{I}_k$ works as a **\"test\" fold**, while the remaining observations' indices constitute a **\"training\" fold**,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03+0.j, -0.05+0.j, -0.  +0.j, ...,  0.  +0.j,  0.  +0.j,\n",
       "         0.  +0.j],\n",
       "       [-0.02+0.j,  0.  +0.j, -0.01+0.j, ...,  0.  +0.j,  0.  +0.j,\n",
       "         0.  +0.j],\n",
       "       [-0.01+0.j, -0.04+0.j, -0.  +0.j, ...,  0.  +0.j,  0.  +0.j,\n",
       "         0.  +0.j],\n",
       "       ...,\n",
       "       [-0.03+0.j,  0.03+0.j,  0.04+0.j, ...,  0.  +0.j,  0.  +0.j,\n",
       "         0.  +0.j],\n",
       "       [-0.04+0.j, -0.05+0.j, -0.02+0.j, ...,  0.  +0.j,  0.  +0.j,\n",
       "         0.  +0.j],\n",
       "       [-0.01+0.j,  0.  +0.j,  0.01+0.j, ...,  0.  +0.j,  0.  +0.j,\n",
       "         0.  +0.j]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "######################### 2. We then split the (randomized) auxiliary observations into K non-overlapping folds of equal size #########################\n",
    "def shuffle_split(data, K):\n",
    "    # Initialize ShuffleSplit\n",
    "    shuffle_split = ShuffleSplit(n_splits=K, test_size=0.2, random_state=42) \n",
    "    # test_size=0.2 : 20% des données pour l'ensemble de test, 80% pour l'ensemble d'entraînement.\n",
    "\n",
    "    # Create empty list to store splits\n",
    "    splits = []\n",
    "\n",
    "    # Perform shuffling and splitting\n",
    "    for train_index, test_index in shuffle_split.split(data.columns):\n",
    "        train_fold = [data.columns[i] for i in train_index]\n",
    "        test_fold = [data.columns[i] for i in test_index]\n",
    "        splits.append((train_fold, test_fold)) ## attention à cette structure\n",
    "\n",
    "    return splits\n",
    "\n",
    "# ---------------------------------------------------------------- TESTS ----------------------------------------------------------------\n",
    "days = 250\n",
    "beta = 0.999\n",
    "X_tilde = auxilary_matrix(days=days, beta=beta, df_cleaned=df_cleaned)\n",
    "splits = shuffle_split(data=X_tilde, K=20)\n",
    "eigenvector = eigen_sample(data=X_tilde, train_fold=splits[0][0])\n",
    "eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a fixed exponential decay rate $\\beta \\in (0, 1)$ an its associated EWA-SC $\\bm{E}$. Remember we denoted $\\bm{\\Sigma}$ the \"true\" and unobserved covariance matrix. Both these matrices are symmetric and thus admit the following spectral decomposition: \n",
    "\n",
    "\\begin{equation}\n",
    "\\bm{E} = \\sum_{i=1}^{n} \\hat{\\lambda}_i \\hat{u}_i \\hat{u}_i', \\quad \\text{and} \\quad \\bm{\\Sigma} = \\sum_{i=1}^{n} \\lambda_i u_i u_i',\n",
    "\\end{equation}\n",
    "\n",
    "where $(\\hat{\\lambda}_1, \\ldots, \\hat{\\lambda}_n; \\hat{u}_1, \\ldots, \\hat{u}_n)$  denotes a system of sample eigenvalues and eigenvectors of $\\bm{E}$, and $(\\lambda_1, \\ldots, \\lambda_n; u_1, \\ldots, u_n)$ denotes a system of eigenvalues and eigenvectors of the \"true\" covariance $\\bm{\\Sigma}$. The eigenvalues are assumed to be sorted in ascending order.\n",
    "\n",
    "To correct the bias previously mentioned, we consider a specific framework where the sample eigenvalues should be corrected while retaining the sample eigenvectors of the original matrix. This is mathematically tantamount to write:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\bm{\\Sigma}} = \\sum_{i=1}^{n} \\xi_i \\hat{u}_i \\hat{u}_i',\n",
    "\\end{equation}\n",
    "\n",
    "where $\\bm{\\xi} = (\\xi_i)_{i=1,...,n}$  is an $n$-dimensional vector that we have to obtain. This framework is somewhat reasonable as, in absence of any **a priori** knowledge about the structure of the covariance matrix, the most natural guess that we have about the population eigenvectors is the sample eigenvectors that we observe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### 3. For each K fold configuration, we estimate the sample eigenvectors from the training set #########################\n",
    "def eigen_sample(data, train_fold): ## we train the data on this test fold\n",
    "\n",
    "    train_data = data.loc[:, train_fold]\n",
    "\n",
    "    # Calculer la moyenne de l'ensemble d'entraînement\n",
    "    mean_train = np.mean(train_data, axis=1)\n",
    "\n",
    "    # Centrer les données d'entraînement\n",
    "    centered_train_data = train_data.sub(mean_train, axis=0)\n",
    "\n",
    "    # Calculer la matrice de covariance des données d'entraînement\n",
    "    cov_matrix_train = np.cov(centered_train_data) ## size number of assets * number of assets\n",
    "\n",
    "    # Calculer les vecteurs et valeurs propres de la matrice de covariance\n",
    "    _, eigenvectors_train = np.linalg.eigh(cov_matrix_train) ## .eigh and not .eig so that the eigenvalues are real \n",
    "\n",
    "    return eigenvectors_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each $K$ fold configuration, we estimate the sample eigenvectors from the training set and then estimate an N-dimensional vector of out-of-sample variances using the test set and the sample eigenvectors. \n",
    "\n",
    "- Finally, we average the out-of-sample variance estimates over $K$ to give us the bias-corrected eigenvalue of the ith sample eigenvector portfolio denoted as $\\xi^{\\dagger}_i$ for all $i$.\n",
    "\n",
    "These two last steps are equivalent to introducing the $K$-fold cross-validation estimator:\n",
    "\n",
    "$$\n",
    "\\xi^{\\dagger}_i := \\frac{1}{K} \\sum_{k=1}^K \\sum_{t \\in \\mathcal{I}_k}  \\frac{1}{\\lvert \\mathcal{I}_k \\rvert} \\left(\\hat{u}_i[k]'\\tilde{x}_t \\right)^2, \\quad \\text{for } i = 1, \\ldots, n,\n",
    "$$\n",
    "\n",
    "where: \n",
    "- $\\lvert \\mathcal{I}_k \\rvert$ denotes the cardinality of the kth test set such that each of them is approximately equal in size, that is, $K \\lvert \\mathcal{I}_k \\rvert \\approx d$\n",
    "- Here, $\\hat{u}_i[k]$ is the $i$-th sample eigenvector of a sample covariance matrix that is obtained from the training fold, and $\\tilde{x}$ is a sample vector of the auxiliary observation matrix from the test fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00017920707091669935+3.6758461401145325e-06j)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def intra_fold_loss(data, test_fold, sample_eigenvector_i, beta): ## we test the data on this test fold\n",
    "\n",
    "    ## 1. get the fold cardinality \n",
    "    fold_cardinality = len(test_fold)\n",
    "\n",
    "    ## 2. sample vector of the auxiliary observation matrix from the test fold (inspired from the code above)\n",
    "\n",
    "    days = len(test_fold)\n",
    "    X = data.loc[:,test_fold].transpose()\n",
    "\n",
    "    ## 2. We slightly adjust the matrix of observations to get the auxiliary matrix that puts more weight on recent dates\n",
    "\n",
    "    W = np.sqrt(np.diag(days * (1 - beta) * beta**(np.arange(days)[::-1]) / (1 - beta**days)))  # Compute the weight matrix\n",
    "    X_tilde = pd.DataFrame(index=X.index, columns=X.columns, data=np.dot(W, X)).transpose()\n",
    "\n",
    "    res = (np.dot(sample_eigenvector_i, X_tilde) ** 2) / fold_cardinality\n",
    "    result = np.sum(res)\n",
    "\n",
    "    return result\n",
    "\n",
    "# ---------------------------------------------------------------- TESTS ----------------------------------------------------------------\n",
    "beta = 0.99\n",
    "data = X_tilde\n",
    "sample_eigenvector_i = eigenvector[0]\n",
    "X = X_tilde.loc[:, splits[0][1]]\n",
    "test_fold = splits[0][1]\n",
    "intra_loss = intra_fold_loss(data=data, test_fold=splits[0][1], sample_eigenvector_i=sample_eigenvector_i, beta=beta)\n",
    "intra_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "average_loss_i() got an unexpected keyword argument 'days'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# ---------------------------------------------------------------- TEST ----------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m average_loss_i \u001b[39m=\u001b[39m average_loss_i(df_cleaned\u001b[39m==\u001b[39;49mdf_cleaned, days\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m, splits\u001b[39m=\u001b[39;49msplits, index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, beta\u001b[39m=\u001b[39;49m\u001b[39m0.99\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: average_loss_i() got an unexpected keyword argument 'days'"
     ]
    }
   ],
   "source": [
    "def average_loss_i(data, splits, index, beta):\n",
    "\n",
    "    res = 0 ## to stock the overall loss\n",
    "\n",
    "    for (train_fold, test_fold) in splits:\n",
    "\n",
    "        ## sur chaque fold, on calcule les sample eigenvectors à partir du training fold correspondant\n",
    "\n",
    "        sample_eigenvector_i = eigen_sample(data=data, train_fold=train_fold)[index] ## on ne garde que l'eigenvector correspondant au bon index\n",
    "\n",
    "        ## sur chaque fold, on calcule la perte au sein du fold à partir de l'échantillon de test\n",
    "\n",
    "        res = res + intra_fold_loss(data=data, test_fold=test_fold, sample_eigenvector_i=sample_eigenvector_i, beta=beta)\n",
    "\n",
    "    res = res / len(splits) ## we average by the number of folds (which corresponds to the lengths of the splits)\n",
    "\n",
    "    return res\n",
    "\n",
    "# ---------------------------------------------------------------- TEST ----------------------------------------------------------------\n",
    "average_loss_i = average_loss_i(df_cleaned==df_cleaned, days=250, splits=splits, index=0, beta=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# ---------------------------------------------------------------- TEST ----------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m eigenvalue_estimator \u001b[39m=\u001b[39m eigenvalue_estimator(df_cleaned\u001b[39m=\u001b[39;49mdf_cleaned, days\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m, K\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, beta\u001b[39m=\u001b[39;49m\u001b[39m0.99\u001b[39;49m)\n",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m splits \u001b[39m=\u001b[39m shuffle_split(data\u001b[39m=\u001b[39mdata, K\u001b[39m=\u001b[39mK)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m number_of_stocks \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([average_loss_i(data\u001b[39m=\u001b[39;49mdata, splits\u001b[39m=\u001b[39;49msplits, index\u001b[39m=\u001b[39;49mi, beta\u001b[39m=\u001b[39;49mbeta) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(number_of_stocks)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m splits \u001b[39m=\u001b[39m shuffle_split(data\u001b[39m=\u001b[39mdata, K\u001b[39m=\u001b[39mK)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m number_of_stocks \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([average_loss_i(data\u001b[39m=\u001b[39;49mdata, splits\u001b[39m=\u001b[39;49msplits, index\u001b[39m=\u001b[39;49mi, beta\u001b[39m=\u001b[39;49mbeta) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(number_of_stocks)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m## to stock the overall loss\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m (train_fold, test_fold) \u001b[39min\u001b[39;00m splits:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m## sur chaque fold, on calcule les sample eigenvectors à partir du training fold correspondant\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     sample_eigenvector_i \u001b[39m=\u001b[39m eigen_sample(data\u001b[39m=\u001b[39;49mdata, train_fold\u001b[39m=\u001b[39;49mtrain_fold)[index] \u001b[39m## on ne garde que l'eigenvector correspondant au bon index\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m## sur chaque fold, on calcule la perte au sein du fold à partir de l'échantillon de test\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     res \u001b[39m=\u001b[39m res \u001b[39m+\u001b[39m intra_fold_loss(data\u001b[39m=\u001b[39mdata, test_fold\u001b[39m=\u001b[39mtest_fold, sample_eigenvector_i\u001b[39m=\u001b[39msample_eigenvector_i, beta\u001b[39m=\u001b[39mbeta)\n",
      "\u001b[1;32m/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m cov_matrix_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcov(centered_train_data) \u001b[39m## size number of assets * number of assets\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Calculer les vecteurs et valeurs propres de la matrice de covariance\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m _, eigenvectors_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49meigh(cov_matrix_train) \u001b[39m## .eigh and not .eig so that the eigenvalues are real \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/khelifanail/Documents/GitHub/Portfolio_clustering_project/Code/EMA_CV.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mreturn\u001b[39;00m eigenvectors_train\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/linalg/linalg.py:1487\u001b[0m, in \u001b[0;36meigh\u001b[0;34m(a, UPLO)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     gufunc \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39meigh_up\n\u001b[1;32m   1486\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->dD\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->dd\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1487\u001b[0m w, vt \u001b[39m=\u001b[39m gufunc(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[1;32m   1488\u001b[0m w \u001b[39m=\u001b[39m w\u001b[39m.\u001b[39mastype(_realType(result_t), copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1489\u001b[0m vt \u001b[39m=\u001b[39m vt\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def eigenvalue_estimator(df_cleaned, days, K, beta):\n",
    "\n",
    "    data = auxilary_matrix(days=days, beta=beta, df_cleaned=df_cleaned)\n",
    "    \n",
    "    splits = shuffle_split(data=data, K=K)\n",
    "\n",
    "    number_of_stocks = data.shape[0]\n",
    "\n",
    "    x = np.array([average_loss_i(data=data, splits=splits, index=i, beta=beta) for i in range(number_of_stocks)])\n",
    "                  \n",
    "    return x\n",
    "\n",
    "# ---------------------------------------------------------------- TEST ----------------------------------------------------------------\n",
    "eigenvalue_estimator = eigenvalue_estimator(df_cleaned=df_cleaned, days=250, K=10, beta=0.99)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
